# Ritter IPO Analysis
#   Step 3: Train and evaluate different binary classifcation models
#           Finally, use Ensemble Model (rx_ensemble) for best results
#
#   This script uses the version of Python bundled with the
#     Microsoft Machine Learning Server version 9.3
#
#   This project is an example end-to-end data science experiment
#     and is not intended to generate meaningful results
#   
#   This file depends on XDF files generated by ritter_ipo_feature_mls.py
#
import numpy as np
import pandas as pd
import revoscalepy as rv
from revoscalepy import rx_import
from revoscalepy import rx_logit
import microsoftml as ml
from microsoftml import FastForest, FastLinear, FastTrees, LogisticRegression, NeuralNetwork
from microsoftml import rx_fast_forest, rx_fast_linear, rx_fast_trees, rx_logistic_regression, rx_neural_network
from microsoftml import rx_ensemble, Ensemble, EnsembleControl
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve
from sklearn.metrics import auc

# Read Cleaned Ipo2609 XDF file
IPO2609FE = rx_import("IPO2609FeatureEngineering.xdf")
ipo_train, ipo_test = train_test_split(IPO2609FE)

# Columns for training (X), remove label (y)
features = IPO2609FE.columns.drop(["underpriced"])

# SciKit-Learn logistic regression model
X = ipo_train[features.values.tolist()]
y = np.ravel(ipo_train["underpriced"])
sk_log_reg = LogisticRegression()
sk_log_model = sk_log_reg.fit(X, y)
probList = []
probList = sk_log_model.predict_proba(ipo_test[features.values.tolist()])[:,1]
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("scikit AUC: " + str(aucResult))

# Define Formula for all Microsoft algorithms
formula = "underpriced ~ " + "+".join(features)

# Revoscalepy rx-logit
rx_logit_model = rx_logit(formula=formula, data=ipo_train)
probArray = rv.rx_predict(rx_logit_model, data=ipo_test)
probList = probArray["underpriced_Pred"].values 
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("rx-logit AUC: " + str(aucResult))


# MicrosoftML Linear Regression
ml_lreg_model = rx_logistic_regression(formula=formula, data=ipo_train)
ml_lreg_score = ml.rx_predict(ml_lreg_model, data=ipo_test,
                     extra_vars_to_write=["underpriced"])
prob_pred = [ml_lreg_score.loc[i, "Probability"] if ml_lreg_score.loc[i, "PredictedLabel"] \
             else (1 - ml_lreg_score.loc[i, "Probability"]) for i in range(0, ml_lreg_score.shape[0])]
good = ml_lreg_score["PredictedLabel"].as_matrix() == (ipo_test["underpriced"] == 1).as_matrix()
fpr, tpr, th = roc_curve(good.ravel(), prob_pred)
aucResult = auc(fpr, tpr)
print ("ml-linear-reg AUC: " + str(aucResult))


# Microsoftml Fast Forest
ml_ff_model = rx_fast_forest(formula=formula, data=ipo_train)
ml_ff_pred = ml.rx_predict(ml_ff_model, data=ipo_test, extra_vars_to_write=["underpriced"])
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"], ml_ff_pred["PredictedLabel"])
aucResult = auc(fpr, tpr)
print ("ml-ff AUC: " + str(aucResult))


# Fast Forest Tune Hyperparameters

# Split training data into A / B
trainA, trainB = train_test_split(ipo_train)

# Function to train and test on A / B.
def train_test_hyperparameter(trainA, trainB, **hyper):
    model = rx_fast_trees(formula=formula, data=trainA, verbose=0, **hyper)
    pred = ml.rx_predict(model, trainB, extra_vars_to_write=["underpriced"])
    conf = confusion_matrix(pred["underpriced"], pred["PredictedLabel"])
    return (conf[0,0] + conf[1,1]) / conf.sum()


# num_leaves hyperparameter
hyper_values = [5, 10, 15, 20, 25, 30, 35, 40, 50, 100, 200]
perfs = []
for val in hyper_values:
    acc = train_test_hyperparameter(trainA, trainB, num_leaves=val)
    perfs.append(acc)
    print("-- Training with hyper={0} performance={1}".format(val, acc))


# Plot num_leaves hyperparameter results
fig, ax = plt.subplots(1, 1)
ax.plot(hyper_values, perfs, "o-")
ax.set_xlabel("num_leaves")
ax.set_ylabel("% correctly classified")
plt.show()

# Choose best (max) result for num_leaves
leaves_results = max(zip(perfs, hyper_values))
print("max={0}".format(leaves_results))

# Tune num_trees
hyper_values = [25, 50, 75, 100, 125, 150, 175, 200, 300, 400, 500]
perfs = []
for val in hyper_values:
    acc = train_test_hyperparameter(trainA, trainB, num_leaves=leaves_results[1], num_trees=val)
    perfs.append(acc)
    print("-- Training with hyper={0} performance={1}".format(val, acc))


# Plot num_trees hyperparameter results
fig, ax = plt.subplots(1, 1)
ax.plot(hyper_values, perfs, "o-")
ax.set_xlabel("num_trees")
ax.set_ylabel("% correctly classified")
plt.show()

# Choose best (max) result for num_trees
trees_results = max(zip(perfs, hyper_values))
print("max={0}".format(trees_results))

ml_ff_hyper_model = rx_fast_trees(formula=formula, data=ipo_train, num_leaves=leaves_results[1], num_trees=trees_results[1])
ml_ff_hyper_pred = ml.rx_predict(ml_ff_hyper_model, data=ipo_test, extra_vars_to_write=["underpriced"])
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"], ml_ff_hyper_pred["PredictedLabel"])
aucResult = auc(fpr, tpr)
print ("ml-ff-hyper AUC: " + str(aucResult))


# Ensemble Model
trainers = [FastTrees(), FastForest(), FastLinear(), NeuralNetwork()]
ml_ens_model = rx_ensemble(formula=formula, data=ipo_train, trainers=trainers, model_count=4, random_seed=42)
ml_ens_pred = ml.rx_predict(ml_ens_model, data=ipo_test, extra_vars_to_write="underpriced")
fpr, tpr, th = roc_curve(ml_ens_pred["underpriced"], ml_ens_pred["Probability"])
aucResult = auc(fpr, tpr)
print ("ml-ensemble AUC: " + str(aucResult))

# Plot ROC
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC Curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title('ROC')
plt.legend(loc="lower right")
plt.show()

# Ensemble Precision-Recall
ml_ens_prfs = precision_recall_fscore_support(ipo_test["underpriced"], ml_ens_pred["PredictedLabel"], average='micro')
print ("ml-ff Precision, Recall, FScore: " + str(ml_ens_prfs))
precision, recall, thresholds = precision_recall_curve(ipo_test["underpriced"], ml_ens_pred["PredictedLabel"])
# Plot Fast Forest Precision-Recall curve
plt.plot(recall, precision)
plt.title('Ensemble Precision-Recall curve')
plt.legend(loc='lower right', fontsize='small')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show() 
