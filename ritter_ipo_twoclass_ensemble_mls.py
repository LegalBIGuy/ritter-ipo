# Ritter IPO Analysis
#   Step 3: Train and evaluate different binary classifcation models
#           Finally, use Ensemble Model (rx_ensemble) for best results
#
#   This script uses the version of Python bundled with the
#     Microsoft Machine Learning Server version 9.3
#
#   This project is an example end-to-end data science experiment
#     and is not intended to generate meaningful results
#   
#   This file depends on XDF files generated by ritter_ipo_feature_mls.py
#
import numpy as np
import pandas as pd
import revoscalepy as rv
from revoscalepy import rx_import
from revoscalepy import rx_logit, rx_btrees
import microsoftml as ml
from microsoftml import FastForest, FastLinear, FastTrees, LogisticRegression, NeuralNetwork
from microsoftml import rx_fast_forest, rx_fast_linear, rx_fast_trees, rx_logistic_regression, rx_neural_network
from microsoftml import rx_ensemble, Ensemble, EnsembleControl
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve
from sklearn.metrics import auc

# Read Cleaned Ipo2609 XDF file
IPO2609FE = rx_import("IPO2609FeatureEngineering.xdf")
ipo_train, ipo_test = train_test_split(IPO2609FE, random_state=42)

# Columns for training (X), remove label (y)
features = IPO2609FE.columns.drop(["underpriced"])

# SciKit-Learn logistic regression model
X = ipo_train[features.values.tolist()]
y = np.ravel(ipo_train["underpriced"])
sk_log_reg = LogisticRegression()
sk_log_model = sk_log_reg.fit(X, y)
probList = []
probList = sk_log_model.predict_proba(ipo_test[features.values.tolist()])[:,1]
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("scikit logistic AUC: " + str(aucResult))

# Define Formula for all Microsoft algorithms
formula = "underpriced ~ " + "+".join(features)

# Revoscalepy rx-logit
rx_logit_model = rx_logit(formula=formula, data=ipo_train)
probArray = rv.rx_predict(rx_logit_model, data=ipo_test)
probList = probArray["underpriced_Pred"].values 
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("rx-logit AUC: " + str(aucResult))

# Plot ROC
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC Curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title('ROC')
plt.legend(loc="lower right")
plt.show()

# Revoscalepy rx_btrees
seed = 0
n_trees = 50
interaction_depth = 4
n_min_obs_in_node = 1
shrinkage = 0.1
bag_fraction = 0.5
distribution = "bernoulli"
rx_btrees_model = rx_btrees(formula=formula, data=ipo_train, loss_function=distribution, n_tree=n_trees, learning_rate=shrinkage,
                sample_rate=bag_fraction, max_depth=interaction_depth, min_bucket=n_min_obs_in_node, seed=seed,
                replace=False, max_num_bins=200)
probArray = rv.rx_predict(rx_btrees_model, data=ipo_test)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("rx-btrees AUC: " + str(aucResult))


# MicrosoftML Logistic Regression
ml_lreg_model = rx_logistic_regression(formula=formula, data=ipo_train)
ml_lreg_score = ml.rx_predict(ml_lreg_model, data=ipo_test,
                     extra_vars_to_write=["underpriced"])
prob_pred = [ml_lreg_score.loc[i, "Probability"] if ml_lreg_score.loc[i, "PredictedLabel"] \
             else (1 - ml_lreg_score.loc[i, "Probability"]) for i in range(0, ml_lreg_score.shape[0])]
good = ml_lreg_score["PredictedLabel"].as_matrix() == (ipo_test["underpriced"] == 1).as_matrix()
fpr, tpr, th = roc_curve(good.ravel(), prob_pred)
aucResult = auc(fpr, tpr)
print ("ml-logistic-reg AUC: " + str(aucResult))


# Microsoftml Fast Forest
ml_ff_model = rx_fast_forest(formula=formula, data=ipo_train)
ml_ff_pred = ml.rx_predict(ml_ff_model, data=ipo_test, extra_vars_to_write=["underpriced"])
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"], ml_ff_pred["PredictedLabel"])
aucResult = auc(fpr, tpr)
print ("ml-ff AUC: " + str(aucResult))


# Scikit Random Forest
rfc = RandomForestClassifier(random_state = 42)
sk_rf_model = rfc.fit(X, y)
probList = []
probList = sk_rf_model.predict_proba(ipo_test[features.values.tolist()])[:,1]
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("scikit Random Forest Classifier AUC: " + str(aucResult))


# Scikit Random Forest Tune Hyperparameters

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 50)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

# Use the random grid to search for best hyperparameters

# Random search of parameters, using 3 cross validation folds
rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, n_jobs = -1, random_state=42)
# Fit the random search model
rfc_random.fit(X, y)

# Review best hyperparameters
rfc_random.best_params_

# Fit a model with the best hyperparameters
rfc_best = RandomForestClassifier(n_estimators = 1600, min_samples_leaf = 1, min_samples_split = 10, max_features = 'sqrt', random_state = 42)
sk_rfc_best_model = rfc_best.fit(X, y)
probList = []
probList = sk_rfc_best_model.predict_proba(ipo_test[features.values.tolist()])[:,1]
probArray = np.asarray(probList)
fpr, tpr, thresholds = roc_curve(ipo_test["underpriced"] , probArray)
aucResult = auc(fpr, tpr)
print ("scikit Random Forest Classifier Tuned Hyperparameters AUC: " + str(aucResult))


# Ensemble Model
trainers = [FastTrees(), FastForest(), FastLinear(), NeuralNetwork()]
ml_ens_model = rx_ensemble(formula=formula, data=ipo_train, trainers=trainers, random_seed=42)
ml_ens_pred = ml.rx_predict(ml_ens_model, data=ipo_test, extra_vars_to_write="underpriced")
fpr, tpr, th = roc_curve(ml_ens_pred["underpriced"], ml_ens_pred["Probability"])
aucResult = auc(fpr, tpr)
print ("ml-ensemble AUC: " + str(aucResult))

# Ensemble Precision-Recall
ml_ens_prfs = precision_recall_fscore_support(ipo_test["underpriced"], ml_ens_pred["PredictedLabel"], average='micro')
print ("ml-ff Precision, Recall, FScore: " + str(ml_ens_prfs))
precision, recall, thresholds = precision_recall_curve(ipo_test["underpriced"], ml_ens_pred["PredictedLabel"])
# Plot Precision-Recall curve
plt.plot(recall, precision)
plt.title('Ensemble Precision-Recall curve')
plt.legend(loc='lower right', fontsize='small')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show() 
